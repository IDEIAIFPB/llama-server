healthCheckTimeout: 600
logLevel: info

macros:
  "server-default": |
    /app/llama-server
    --threads 8
    --prio 3
    --port 9000
    --cache-type-k q8_0
    --cache-type-v q8_0
    --flash-attn
    --no-webui
    --jinja
  
  "smolvlm-config": |
    /app/llama-server
    --threads 8
    --prio 3
    --port 9000
    --flash-attn
    --no-webui

  "gemma3-args": |
    --temp 1.0
    --repeat-penalty 1.0
    --min-p 0.01
    --top-k 64
    --top-p 0.95
  
  "qwen2_5-args": |
    --temp 0.7
    --top-p 0.8
    --top-k 20
    --min-p 0.01

  "qwen2_5-coder-args": |
    --temp 0.2
    --top-p 0.8
    --top-k 20
    --min-p 0.1
  
  "qwq-args": |
    --temp 0.6
    --repeat-penalty 1.1
    --dry-multiplier 0.5
    --top-p 0.95
    --top-k 40
    --min-p 0.01
    --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"
    --reasoning-format deepseek

  "qwen3-args": |
    --temp 0.6
    --top-p 0.95
    --top-k 20
    --min-p 0.01
    --reasoning-format deepseek
  
  "phi4-reasoning-args": |
    --temp 0.8
    --top-p 0.95
    --min-p 0.00
    --reasoning-format deepseek

  "llama3-args": |
    --temp 0.6
    --top-p 0.95
    --top-k 20
    --min-p 0.01

  "llama4-args": |
    --temp 0.6
    --top-p 0.9
    --top-k 20
    --min-p 0.01
  
  "devstral-args": |
    --temp 0.15
    --top-p 0.95
    --top-k 64
    --min-p 0.01
    --repeat-penalty 1.0

  "draft-model-args": |
    --draft-max 16
    --draft-min 4
    --draft-p-min 0.4

  "qwen1_5b-draft-model-args": |
    --draft-max 8
    --draft-min 1
    --draft-p-min 0.4

models:
  # Text Only
  Qwen2.5-7B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-7B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --split-mode none
      --device CUDA0
      --ctx-size 32768
      --gpu-layers 49
    ttl: 1200

  Qwen2.5-Coder-14B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-14B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --device CUDA0
      --ctx-size 32768
      --gpu-layers 49
    ttl: 1200
  
  Qwen2.5-Coder-14B-Draft-0.5B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-14B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --device CUDA0
      --ctx-size 32768
      --gpu-layers 49
      --model-draft /app/models/Qwen2.5-Coder-0.5B-Instruct-IQ4_XS.gguf
      ${draft-model-args}
      --device-draft CUDA1
      --ctx-size-draft 32768
      --gpu-layers-draft 99
    ttl: 1200

  Qwen2.5-Coder-14B-Draft-1.5B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-14B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --device CUDA0
      --ctx-size 32768
      --gpu-layers 49
      --model-draft /app/models/Qwen2.5-Coder-1.5B-Instruct-Q6_K_L.gguf
      ${qwen1_5b-draft-model-args}
      --device-draft CUDA1
      --ctx-size-draft 32768
      --gpu-layers-draft 99
    ttl: 1200

  Qwen2.5-Coder-32B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 32768
      --gpu-layers 65
    ttl: 1200

  Qwen2.5-Coder-32B-Draft-0.5B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --split-mode row
      --ctx-size 30000
      --gpu-layers 65
      --model-draft /app/models/Qwen2.5-Coder-0.5B-Instruct-IQ4_XS.gguf
      ${draft-model-args}
      --ctx-size-draft 30000
      --gpu-layers-draft 99
    ttl: 1200

  Qwen2.5-Coder-32B-Draft-1.5B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf
      ${qwen2_5-coder-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 20000
      --gpu-layers 65
      --model-draft /app/models/Qwen2.5-Coder-1.5B-Instruct-Q6_K_L.gguf
      ${qwen1_5b-draft-model-args}
      --ctx-size-draft 20000
      --gpu-layers-draft 99
    ttl: 1200

  QwQ-32B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-4B-128K-UD-Q4_K_XL.gguf
      ${qwq-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 32000
      --gpu-layers 65
    ttl: 1200

  Qwen3-4B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-4B-128K-UD-Q4_K_XL.gguf
      ${qwen3-args}
      --split-mode none
      --device CUDA0
      --ctx-size 100000
      --gpu-layers 37
    ttl: 1200

  Qwen3-4B-128K:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-4B-128K-UD-Q4_K_XL.gguf
      ${qwen3-args}
      --split-mode row
      --tensor-split 80,20
      --ctx-size 131072
      --gpu-layers 37
    ttl: 1200
  
  Qwen3-8B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-8B-UD-Q4_K_XL.gguf
      ${qwen3-args}
      --split-mode none
      --device CUDA0
      --ctx-size 32768
      --gpu-layers 37
    ttl: 1200

  Qwen3-32B-Q3:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-32B-128K-UD-IQ3_XXS.gguf
      ${qwen3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 70000
      --gpu-layers 65
    ttl: 1200
  
  Qwen3-32B-Q3-Draft-1.7B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-32B-128K-UD-IQ3_XXS.gguf
      ${qwen3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 28000
      --gpu-layers 65
      --model-draft /app/models/Qwen3-1.7B-UD-Q4_K_XL.gguf
      ${draft-model-args}
      --ctx-size-draft 28000
      --gpu-layers-draft 99
    ttl: 1200
  
  Qwen3-32B-Q3-Draft-0.6B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-32B-128K-UD-IQ3_XXS.gguf
      ${qwen3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 28000
      --gpu-layers 65
      --model-draft /app/models/Qwen3-0.6B-UD-Q8_K_XL.gguf
      ${draft-model-args}
      --ctx-size-draft 28000
      --gpu-layers-draft 99
    ttl: 1200

  Qwen3-30B-A3B-Q3:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen3-30B-A3B-128K-UD-IQ3_XXS.gguf
      ${qwen3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 131072
      --gpu-layers 49
    ttl: 1200
  
  Gemma3-4B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-4B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --split-mode none
      --device CUDA0
      --ctx-size 131072
      --gpu-layers 35
    ttl: 1200
  
  Gemma3-12B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-12B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --split-mode none
      --device CUDA0
      --ctx-size 95000
      --gpu-layers 49
    ttl: 1200

  Gemma3-12B-Draft-1B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-12B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --device CUDA0
      --ctx-size 95000
      --gpu-layers 49
      --model-draft /app/models/Gemma3-1B-Instruct-UD-Q6_K_XL.gguf
      ${draft-model-args}
      --device-draft CUDA1
      --ctx-size-draft 95000
      --gpu-layers-draft 99
    ttl: 1200

  Gemma3-27B-Q3:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-27B-Instruct-UD-IQ3_XXS.gguf
      ${gemma3-args}
      --split-mode none
      --device CUDA0
      --ctx-size 4096
      --gpu-layers 63
    ttl: 1200
  
  Gemma3-27B-Q3-128K:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-27B-Instruct-UD-IQ3_XXS.gguf
      ${gemma3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 131072
      --gpu-layers 63
    ttl: 1200

  Gemma3-27B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-27B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 85000
      --gpu-layers 63
    ttl: 1200

  Gemma3-27B-128K:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-27B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 100000
      --gpu-layers 63
    ttl: 1200

  Gemma3-27B-Draft-1B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-27B-Instruct-UD-Q4_K_XL.gguf
      ${gemma3-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 50000
      --gpu-layers 63
      --model-draft /app/models/Gemma3-1B-Instruct-UD-Q6_K_XL.gguf
      ${draft-model-args}
      --ctx-size-draft 50000
      --gpu-layers-draft 99
    ttl: 1200

  Llama4-Scout-17B-16E:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Llama-4-Scout-17B-16E-Instruct-UD-IQ3_XXS.gguf
      ${llama4-args}
      --split-mode none
      --device CUDA0
      --ctx-size 32000
      --gpu-layers 99
      --batch-size 2048
      --ubatch-size 2048
      --override-tensor "([0-9]+).ffn_.*_exps.=CPU"
    ttl: 1200

  Phi4:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Phi-4-Q4_K_M.gguf
      --split-mode none
      --device CUDA0
      --ctx-size 16384
      --gpu-layers 99
    ttl: 1200

  Phi4-Reasoning:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Phi-4-Reasoning-UD-Q4_K_XL.gguf
      ${phi4-reasoning-args}
      --split-mode none
      --device CUDA0
      --ctx-size 16384
      --gpu-layers 99
    ttl: 1200

  Phi4-Reasoning-Plus:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Phi-4-Reasoning-Plus-UD-Q4_K_XL.gguf
      ${phi4-reasoning-args}
      --split-mode none
      --device CUDA0
      --ctx-size 16384
      --gpu-layers 99
    ttl: 1200

  Devstral-Small:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Devstral-Small-2505-UD-Q4_K_XL.gguf
      ${devstral-args}
      --split-mode row
      --tensor-split 50,50
      --ctx-size 70000
      --gpu-layers 99
    ttl: 1200
  
  Mistral-Small-3.1-24B-Instruct-Q3:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Mistral-Small-3.1-24B-Instruct-2503-UD-IQ3_XXS.gguf
      --split-mode none
      --device CUDA0
      --ctx-size 16384
      --gpu-layers 99
    ttl: 1200
  
  Mistral-Small-3.1-24B-Instruct:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Mistral-Small-3.1-24B-Instruct-2503-UD-Q4_K_XL.gguf
      --split-mode row
      --tensor-split 50,50
      --ctx-size 70000
      --gpu-layers 99
    ttl: 1200
  
  Llama3.1-8B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Llama-3.1-8B-Instruct-UD-Q4_K_XL.gguf
      --split-mode none
      --device CUDA0
      --ctx-size 80000
      --gpu-layers 99
    ttl: 1200

  # Embeddings
  # https://github.com/ggml-org/llama.cpp/pull/14029
  # "Qwen3-Embedding-0.6B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Embedding-0.6B-Q8_0.gguf
  #     --ctx-size 32768
  #     --device CUDA0
  #     --gpu-layers 99
  #     --embeddings
  
  # "Qwen3-Embedding-4B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Embedding-4B-Q8_0.gguf
  #     --ctx-size 32768
  #     --device CUDA0
  #     --gpu-layers 99
  #     --embeddings
  
  # "Qwen3-Embedding-8B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Embedding-8B-Q4_K_M.gguf
  #     --ctx-size 32768
  #     --device CUDA0
  #     --gpu-layers 99
  #     --embeddings
  
  "Multilingual-E5-Large":
    proxy: http://127.0.0.1:9000
    cmd: |
      ${server-default}
      -m /app/models/multilingual-e5-large-instruct-q8_0.gguf
      --device CUDA0
      --gpu-layers 99
      --embeddings

  # Reranker
  "BGE-Reranker-V2-M3":
    proxy: http://127.0.0.1:9000
    cmd: |
      ${server-default}
      -m /app/models/BGE-Reranker-V2-M3-Q8.gguf
      --device CUDA0
      --gpu-layers 99
      --reranking

  # "Qwen3-Reranker-0.6B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Reranker-0.6B.Q8_0.gguf
  #     --device CUDA0
  #     --gpu-layers 99
  #     --reranking
  
  # "Qwen3-Reranker-4B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Reranker-4B.Q8_0.gguf
  #     --device CUDA0
  #     --gpu-layers 99
  #     --reranking

  # "Qwen3-Reranker-8B":
  #   proxy: http://127.0.0.1:9000
  #   cmd: |
  #     ${server-default}
  #     -m /app/models/Qwen3-Embedding-8B-Q4_K_M.gguf
  #     --device CUDA0
  #     --gpu-layers 99
  #     --reranking

  # Multimodal
  Gemma3-4B-Gaia:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Gemma3-Gaia-PT-BR-4b-it.Q6_K.gguf
      --device CUDA0
      --ctx-size 131072
      --mmproj /app/models/Gemma3-Gaia-PT-BR-4b-it.mmproj-Q8_0.gguf
      --gpu-layers 99
    ttl: 1200
  
  Qwen2.5-VL-32B:
    proxy: http://127.0.0.1:9000
    cmd: >
      ${server-default}
      --model /app/models/Qwen2.5-VL-32B-Instruct-UD-IQ3_XXS.gguf
      --split-mode row
      --ctx-size 24000
      --mmproj /app/models/Qwen2.5-VL-32B-Instruct.mmproj-F16.gguf
      --gpu-layers 99
    ttl: 1200

  SmolVLM-256M:
    proxy: http://127.0.0.1:9000
    env:
      - "CUDA_VISIBLE_DEVICES=0"
    cmd: >
      ${smolvlm-config}
      --model /app/models/SmolVLM-256M.gguf
      --mmproj /app/models/SmolVLM-256M.mmproj.gguf
      --ctx-size 8192
      --gpu-layers 99
      --chat-template smolvlm
    ttl: 1200
